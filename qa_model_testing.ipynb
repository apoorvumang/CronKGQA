{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TKBCModel' from 'models' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d3065b32afa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtkbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporalDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTKBCOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIKBCOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComplEx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTComplEx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTNTComplEx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratche/home/apoorv/Temporal_KGQA/tkbc/datasets.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTKBCModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# todo: this is hardcoded for wikidata_small\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TKBCModel' from 'models' (unknown location)"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import Dict\n",
    "import logging\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from tkbc.datasets import TemporalDataset\n",
    "from optimizers import TKBCOptimizer, IKBCOptimizer\n",
    "from models import ComplEx, TComplEx, TNTComplEx\n",
    "from regularizers import N3, Lambda3\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(1,2), (3,1), (2,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 2), (3, 1)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 1]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[1] for x in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tkbc model\n",
      "tkbc model loaded\n",
      "predictHead 8408 8603 0.9773334883180286\n",
      "predictTime 12224 12272 0.9960886571056062\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import Dict\n",
    "import logging\n",
    "import torch\n",
    "from torch import optim\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from datasets import TemporalDataset\n",
    "from models import ComplEx, TComplEx, TNTComplEx\n",
    "from regularizers import N3, Lambda3\n",
    "from qa_models import QA_model\n",
    "\n",
    "import utils\n",
    "\n",
    "class Args:\n",
    "    dataset = 'wikidata_small'\n",
    "    model = 'TComplEx'\n",
    "    max_epochs = 50\n",
    "    valid_freq = 5\n",
    "    rank = 256\n",
    "    batch_size = 512\n",
    "    learning_rate = 1e-2\n",
    "    emb_reg = 0.01\n",
    "    time_reg = 0.01\n",
    "    no_time_emb = False\n",
    "    \n",
    "args=Args()\n",
    "\n",
    "dataset = TemporalDataset(args.dataset)\n",
    "sizes = dataset.get_shape()\n",
    "tkbc_model = TComplEx(sizes, args.rank, no_time_emb=args.no_time_emb)\n",
    "tkbc_model = tkbc_model.cuda()\n",
    "print('loading tkbc model')\n",
    "\n",
    "# load model\n",
    "# path = 'model2.ckpt'\n",
    "path = 'model_tkbc_60kent.ckpt'\n",
    "tkbc_model.load_state_dict(torch.load(path))\n",
    "print('tkbc model loaded')\n",
    "tkbc_model.eval()\n",
    "\n",
    "fname = '/scratche/home/apoorv/tempqa/data/questions/questions_position_held_small_1_paraphrases_shuffled.pickle'\n",
    "\n",
    "questions = pickle.load(open(fname, 'rb'))\n",
    "\n",
    "all_dicts = utils.getAllDicts()\n",
    "\n",
    "for question_type in ['predictHead', 'predictTime']:\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    k = 1 # hit at k\n",
    "    for i in range(len(questions)):\n",
    "        question_template = questions[i]['template']\n",
    "        if question_type == 'predictHead':\n",
    "            which_question_function = utils.predictHead\n",
    "            target_template = 'Who was the {tail} in {time}?'\n",
    "        elif question_type == 'predictTime':\n",
    "            which_question_function = utils.predictTime\n",
    "            target_template = 'When did {head} hold the position of {tail}?'            \n",
    "        if question_template != target_template:\n",
    "            continue\n",
    "        total_count += 1\n",
    "        id = i   \n",
    "        predicted = which_question_function(questions[id], tkbc_model, all_dicts, k)\n",
    "        intersection_set = questions[id]['answers'].intersection(predicted)\n",
    "        if len(intersection_set) > 0:\n",
    "            correct_count += 1\n",
    "    \n",
    "    print(question_type, correct_count, total_count, correct_count/total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pkg_resources\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "# from qa_models import QA_model\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "# warning: padding id 0 is being used, can have issue like in Tucker\n",
    "# however since so many entities (and timestamps?), it may not pose problem\n",
    "\n",
    "class QA_Dataset(object):\n",
    "    def __init__(self, \n",
    "                filename='/scratche/home/apoorv/tempqa/data/questions/questions_position_held_small_with_paraphrases_v2_shuffled.pickle'):\n",
    "        num_valid = 500\n",
    "        num_test = 500\n",
    "        questions = pickle.load(open(filename, 'rb'))\n",
    "        self.all_dicts = utils.getAllDicts()\n",
    "        self.valid = questions[:num_valid]\n",
    "        self.test = questions[num_valid: num_valid + num_test]\n",
    "        self.train = questions[num_valid + num_test :]\n",
    "        print('Total questions = ', len(questions))\n",
    "\n",
    "        self.data = {}\n",
    "        self.data['valid'] = self.valid\n",
    "        self.data['train'] = self.train\n",
    "        self.data['test'] = self.test\n",
    "\n",
    "    # todo: implement this\n",
    "    def getOrderedEntities(self, question):\n",
    "        return list(question['entities'])\n",
    "\n",
    "    # todo: implement this\n",
    "    def getOrderedTimes(self, question):\n",
    "        return list(question['times'])\n",
    "\n",
    "    def entitiesToIds(self, entities):\n",
    "        output = []\n",
    "        ent2id = self.all_dicts['ent2id']\n",
    "        for e in entities:\n",
    "            output.append(ent2id[e])\n",
    "        return output\n",
    "    \n",
    "    def idToEntTime(self, id):\n",
    "        type = self.getIdType(id)\n",
    "        if type == 'entity':\n",
    "            return self.all_dicts['id2ent'][id]\n",
    "        else:\n",
    "            return self.all_dicts['id2ts'][id]\n",
    "        \n",
    "    def getIdType(self, id):\n",
    "        if id < len(self.all_dicts['ent2id']):\n",
    "            return 'entity'\n",
    "        else:\n",
    "            return 'time'\n",
    "    \n",
    "    def getEntityToText(self, entity_wd_id):\n",
    "        return self.all_dicts['wd_id_to_text'][entity_wd_id]\n",
    "    \n",
    "    def getEntityIdToText(self, id):\n",
    "        ent = self.all_dicts['id2ent'][id]\n",
    "        return self.getEntityToText(ent)\n",
    "\n",
    "    def timesToIds(self, times):\n",
    "        output = []\n",
    "        ts2id = self.all_dicts['ts2id']\n",
    "        for t in times:\n",
    "            output.append(ts2id[(t, 0, 0)])\n",
    "        return output\n",
    "\n",
    "    # from pytorch Transformer:\n",
    "    # If a BoolTensor is provided, the positions with the value of True will be ignored \n",
    "    # while the position with the value of False will be unchanged.\n",
    "    # \n",
    "    # so we want to pad with True\n",
    "    def padding_tensor(self, sequences):\n",
    "        \"\"\"\n",
    "        :param sequences: list of tensors\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        num = len(sequences)\n",
    "        max_len = max([s.size(0) for s in sequences])\n",
    "        out_dims = (num, max_len)\n",
    "        out_tensor = sequences[0].data.new(*out_dims).fill_(0)\n",
    "        # mask = sequences[0].data.new(*out_dims).fill_(0)\n",
    "        mask = torch.ones((num, max_len), dtype=torch.bool) # fills with True\n",
    "        for i, tensor in enumerate(sequences):\n",
    "            length = tensor.size(0)\n",
    "            out_tensor[i, :length] = tensor\n",
    "            mask[i, :length] = False # fills good area with False\n",
    "        return out_tensor, mask\n",
    "    \n",
    "    def toOneHot(self, indices, vec_len):\n",
    "        indices = torch.LongTensor(indices).cuda()\n",
    "        one_hot = torch.FloatTensor(vec_len).cuda()\n",
    "        one_hot.zero_()\n",
    "        one_hot.scatter_(0, indices, 1)\n",
    "        return one_hot\n",
    "\n",
    "    # def process_and_save_data(self, split):\n",
    "    #     data = self.data[split]\n",
    "    #     question_text, entities_times_padded, entities_times_padded_mask, answers_khot = self.process_data(data)\n",
    "    #     print('Done for ', len(data))\n",
    "    #     exit(0)\n",
    "    #     return\n",
    "\n",
    "    def process_data(self, data):\n",
    "        question_text = []\n",
    "        entity_time_ids = []\n",
    "        \n",
    "        num_total_entities = len(self.all_dicts['ent2id'])\n",
    "        num_total_times = len(self.all_dicts['ts2id'])\n",
    "        answers_khot = []\n",
    "\n",
    "        for question in data:\n",
    "            question_text.append(question['paraphrases'][0])\n",
    "            # question_text.append(question['template']) # todo: this is incorrect\n",
    "            et_id = []\n",
    "            entity_ids = self.entitiesToIds(self.getOrderedEntities(question))\n",
    "            time_ids = self.timesToIds(self.getOrderedTimes(question))\n",
    "            # adding num_total_entities to each time id\n",
    "            for i in range(len(time_ids)):\n",
    "                time_ids[i] += num_total_entities\n",
    "            et_id = entity_ids + time_ids # todo: maybe we want ordering as is in question? here entities first, time 2nd\n",
    "            entity_time_ids.append(torch.tensor(et_id, dtype=torch.long))\n",
    "            if question['answer_type'] == 'entity':\n",
    "                answers = self.entitiesToIds(question['answers'])\n",
    "            else:\n",
    "                # adding num_total_entities to each time id\n",
    "                answers = [x + num_total_entities for x in self.timesToIds(question['answers'])]\n",
    "            answers_khot.append(self.toOneHot(answers, num_total_entities + num_total_times))\n",
    "\n",
    "        entities_times_padded, entities_times_padded_mask = self.padding_tensor(entity_time_ids)\n",
    "        answers_khot = torch.stack(answers_khot)\n",
    "\n",
    "        return question_text, entities_times_padded, entities_times_padded_mask, answers_khot\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_batch(self, split='train', start_index=0, batch_size=50):\n",
    "        # just example\n",
    "        return self.process_data(self.data[split][start_index: start_index + batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from models import TComplEx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# training data: questions\n",
    "# model:\n",
    "# 1. tkbc model embeddings (may or may not be frozen)\n",
    "# 2. question sentence embeddings (may or may not be frozen)\n",
    "# 3. linear layer to project question embeddings (unfrozen)\n",
    "# 4. transformer that takes these embeddings (unfrozen) (cats them along a dimension, also takes a mask)\n",
    "# 5. average output embeddings of transformer or take last token embedding?\n",
    "# 6. linear projection of this embedding to tkbc embedding dimension\n",
    "# 7. score with all possible entities/times and sigmoid\n",
    "# or\n",
    "# 7. directly project to dimension num_entity + num_time and sigmoid\n",
    "# 8. BCE loss (multiple correct possible)\n",
    "\n",
    "\n",
    "class QA_model(nn.Module):\n",
    "    def __init__(self, tkbc_model):\n",
    "        super().__init__()\n",
    "        self.st_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "        self.tkbc_embedding_dim = tkbc_model.embeddings[0].weight.shape[1]\n",
    "        self.sentence_embedding_dim = 768 # hardwired from sentence_transformers?\n",
    "\n",
    "        # transformer\n",
    "        self.transformer_dim = self.tkbc_embedding_dim # keeping same so no need to project embeddings\n",
    "        self.nhead = 8\n",
    "        self.num_layers = 6\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.transformer_dim, nhead=self.nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=self.num_layers)\n",
    "\n",
    "        self.project_sentence_to_transformer_dim = nn.Linear(self.sentence_embedding_dim, self.transformer_dim)\n",
    "        # not needed:\n",
    "        # self.project_tkbc_to_transformer_dim = nn.Linear(self.tkbc_embedding_dim, self.transformer_dim)\n",
    "        \n",
    "        # creating combined embedding of time and entities (entities come first)\n",
    "        num_entities = tkbc_model.embeddings[0].weight.shape[0]\n",
    "        num_times = tkbc_model.embeddings[2].weight.shape[0]\n",
    "        ent_emb_matrix = tkbc_model.embeddings[0].weight.data\n",
    "        time_emb_matrix = tkbc_model.embeddings[2].weight.data\n",
    "        full_embed_matrix = torch.cat([ent_emb_matrix, time_emb_matrix], dim=0)\n",
    "        self.entity_time_embedding = nn.Embedding(num_entities + num_times, self.tkbc_embedding_dim)\n",
    "        self.entity_time_embedding.weight.data.copy_(full_embed_matrix)\n",
    "        self.entity_time_embedding.weight.requires_grad = False\n",
    "        self.loss = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "#         self.final_linear = nn.Linear(self.transformer_dim, num_entities + num_times)\n",
    "        return\n",
    "\n",
    "    def forward(self, question_text, entities_times_padded, entities_times_padded_mask):\n",
    "        entity_time_embedding = self.entity_time_embedding(entities_times_padded)\n",
    "        question_embedding = torch.from_numpy(self.st_model.encode(question_text)).cuda()\n",
    "        question_embedding = self.project_sentence_to_transformer_dim(question_embedding)\n",
    "        question_embedding = question_embedding.unsqueeze(1)\n",
    "        sequence = torch.cat([question_embedding, entity_time_embedding], dim=1)\n",
    "        sequence = torch.transpose(sequence, 0, 1)\n",
    "        batch_size = len(question_text)\n",
    "        false_vector = torch.zeros((batch_size, 1), dtype=torch.bool).cuda() # fills with True\n",
    "        mask = torch.cat([false_vector, entities_times_padded_mask], dim=1)\n",
    "        output = self.transformer_encoder(sequence, src_key_padding_mask=mask)\n",
    "        output = torch.transpose(output, 0, 1)\n",
    "        # summing token embeddings\n",
    "        output = torch.sum(output, dim=1)\n",
    "        # now we can either project output to final dim, or we can take dot-product with\n",
    "        # entity/time embedding weight matrix\n",
    "        scores = torch.matmul(output, self.entity_time_embedding.weight.data.T)\n",
    "#         scores = self.final_linear(output)\n",
    "        # scores = torch.sigmoid(scores)\n",
    "        return scores\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnswersFromScores(scores, dataset):\n",
    "    val, ind = torch.topk(scores, 10, largest=True)\n",
    "    predict = ind[0]\n",
    "    answers = []\n",
    "    for a_id in predict:\n",
    "        a_id = a_id.item()\n",
    "        type = dataset.getIdType(a_id)\n",
    "        if type == 'entity':\n",
    "            answers.append(dataset.getEntityIdToText(a_id))\n",
    "        else:\n",
    "            time_id = a_id - len(dataset.all_dicts['ent2id'])\n",
    "            time = dataset.all_dicts['id2ts'][time_id]\n",
    "            answers.append(time[0])\n",
    "    return answers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions =  57954\n"
     ]
    }
   ],
   "source": [
    "questions_file = '/scratche/home/apoorv/tempqa/data/questions/questions_position_held_small_1_paraphrases_shuffled.pickle'\n",
    "dataset = QA_Dataset(questions_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded qa model from  qa_model_frozen_3.ckpt\n"
     ]
    }
   ],
   "source": [
    "qa_model = QA_model(tkbc_model)\n",
    "path = 'qa_model_frozen_3.ckpt'\n",
    "qa_model.load_state_dict(torch.load(path))\n",
    "print('Loaded qa model from ', path)\n",
    "\n",
    "qa_model = qa_model.cuda()\n",
    "optimizer = torch.optim.Adam([param for param in qa_model.parameters() if param.requires_grad == True], lr=lr)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch loss 1.3238293676404282\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "qa_model.train()\n",
    "for epoch in range(1):\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(dataset.train)// batch_size):\n",
    "#     for i in range(1):\n",
    "        qa_model.zero_grad()\n",
    "        question_text, entities_times_padded, entities_times_padded_mask, answers_khot = dataset.get_batch(\n",
    "            split='train',\n",
    "            start_index = i*batch_size,\n",
    "            batch_size=batch_size)\n",
    "        scores = qa_model.forward(question_text, entities_times_padded.cuda(), entities_times_padded_mask.cuda())\n",
    "        loss = qa_model.loss(scores, answers_khot.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print('epoch loss', epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model.eval()\n",
    "start_index = 0\n",
    "split = 'valid'\n",
    "\n",
    "question_text, entities_times_padded, entities_times_padded_mask, answers_khot = dataset.get_batch(\n",
    "    split=split,\n",
    "    start_index=start_index,\n",
    "    batch_size=1)\n",
    "scores = qa_model.forward(question_text, entities_times_padded.cuda(), entities_times_padded_mask.cuda())\n",
    "loss = qa_model.loss(scores, answers_khot.cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0015, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When was the first time that Henry George Boldero was the Member of the 14th Parliament of the United Kingdom?']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When was the first time that Q5721867 was the Q41582553?',\n",
       " 'answers': {1841},\n",
       " 'answer_type': 'time',\n",
       " 'template': 'When was the {adj} time that {head} was the {tail}?',\n",
       " 'entities': {'Q41582553', 'Q5721867'},\n",
       " 'times': set(),\n",
       " 'relations': {'P39'},\n",
       " 'paraphrases': ['When was the first time that Henry George Boldero was the Member of the 14th Parliament of the United Kingdom?']}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data['valid'][start_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(61071, device='cuda:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(answers_khot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26196, 35754, 13940, 61204, 23946, 61226, 61228, 61235, 61231, 61236],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val, ind = torch.topk(scores, 10, largest=True)\n",
    "ind[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ann Taylor, Baroness Taylor of Bolton',\n",
       " 'Porter Goss',\n",
       " 'John Hume',\n",
       " 1974,\n",
       " 'Bill Richardson',\n",
       " 1996,\n",
       " 1998,\n",
       " 2005,\n",
       " 2001,\n",
       " 2006]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getAnswersFromScores(scores, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(answers_khot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21322"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_dicts['id2ent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
