{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enclosed-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Dict\n",
    "import logging\n",
    "import torch\n",
    "from torch import optim\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from qa_models import QA_model, QA_model_Only_Embeddings, QA_model_BERT, QA_model_EaE, QA_model_EmbedKGQA, QA_model_EaE_replace, QA_model_EmbedKGQA_complex\n",
    "from qa_datasets import QA_Dataset, QA_Dataset_model1, QA_Dataset_EaE, QA_Dataset_EmbedKGQA, QA_Dataset_EaE_replace\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "from utils import loadTkbcModel, loadTkbcModel_complex\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "actual-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "illegal-friendship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complex tkbc model from models/wikidata_big/kg_embeddings/tkbc_model_17dec.ckpt\n",
      "Number ent,rel,ts from loaded model: 125726 406 9621\n",
      "Loaded complex tkbc model\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'wikidata_big'\n",
    "tkbc_model_file = 'tkbc_model_17dec.ckpt'\n",
    "tkbc_model = loadTkbcModel_complex('models/{dataset_name}/kg_embeddings/{tkbc_model_file}'.format(\n",
    "    dataset_name = dataset_name, tkbc_model_file=tkbc_model_file\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "federal-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    lm_frozen = 1\n",
    "    frozen = 1\n",
    "def openFileAsDict(filename):\n",
    "    f = open(filename, 'r')\n",
    "    out = {}\n",
    "    for line in f:\n",
    "        line = line[:-1].split('\\t') # can't strip() since name can be whitespace\n",
    "        out[line[0]] = line[1]\n",
    "    return out\n",
    "\n",
    "def convertToDataPoint(question_text, entities, times, answer_type='entity', answers=set()):\n",
    "    question = {}\n",
    "    question['question'] = question_text\n",
    "#     question['answers'] = answers\n",
    "    question['answers'] = set(['Q888504'])\n",
    "    question['answer_type'] = answer_type\n",
    "    question['entities'] = set(entities)\n",
    "    question['times'] = set(times)\n",
    "    entFile = 'data/wikidata_big/kg/wd_id2entity_text.txt'\n",
    "    id2ent = openFileAsDict(entFile)\n",
    "    paraphrase = question_text\n",
    "    for e in entities:\n",
    "        paraphrase = paraphrase.replace(e, id2ent[e])\n",
    "    question['paraphrases'] = [paraphrase]\n",
    "    return question\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "potential-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "entFile = 'data/wikidata_big/kg/wd_id2entity_text.txt'\n",
    "id2ent = openFileAsDict(entFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "communist-hanging",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/267967963 [00:00<?, ?B/s]\u001b[A\u001b[ACouldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin' to download pretrained weights.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/tempfile.py\u001b[0m in \u001b[0;36mfunc_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m             \u001b[0;31m# Avoid closing the file as long as the wrapper is alive,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-243edc30760f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mqa_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQA_model_EmbedKGQA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtkbc_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m filename = 'models/{dataset_name}/qa_models/{model_file}.ckpt'.format(\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'embedkgqa_dual_frozen_lm_fix_order_ce'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratche/home/apoorv/test_installs/test/Temporal_KGQA/qa_models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tkbc_model, args)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m768\u001b[0m \u001b[0;31m# hardwired from roberta?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'distilbert-base-uncased'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_frozen\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m                             \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_model_archive_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                             archive_file))\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading weights file {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;31m# redirect to the cache, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                 \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_model_archive_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'http'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'https'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# File, and it exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies)\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0mmeta_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"removing temp file %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/tempfile.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc, value, tb)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;31m# deleted when used in a with statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "qa_model = QA_model_EmbedKGQA(tkbc_model, args)\n",
    "filename = 'models/{dataset_name}/qa_models/{model_file}.ckpt'.format(\n",
    "    dataset_name=dataset_name,\n",
    "    model_file='embedkgqa_dual_frozen_lm_fix_order_ce'\n",
    ")\n",
    "print('Loading model from', filename)\n",
    "qa_model.load_state_dict(torch.load(filename))\n",
    "print('Loaded qa model from ', filename)\n",
    "qa_model = qa_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = QA_Dataset_EmbedKGQA(split='valid', dataset_name=dataset_name)\n",
    "original_dataset = QA_Dataset_EmbedKGQA(split='valid', dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "stuck-territory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(qa_model, dataset, batch_size = 128, split='valid', k=10):\n",
    "    num_workers = 4\n",
    "    qa_model.eval()\n",
    "    eval_log = []\n",
    "    k_for_reporting = k # not change name in fn signature since named param used in places\n",
    "    # k_list = [1, 3, 10]\n",
    "    # k_list = [1, 10]\n",
    "    k_list = [1, 5]\n",
    "    max_k = max(k_list)\n",
    "    eval_log.append(\"Split %s\" % (split))\n",
    "    print('Evaluating split', split)\n",
    "\n",
    "    # id = 13799        \n",
    "    ids = [0]\n",
    "    prepared_data = {}\n",
    "    for k, v in dataset.prepared_data.items():\n",
    "        prepared_data[k] = [v[i] for i in ids]\n",
    "    dataset.prepared_data = prepared_data\n",
    "    dataset.data = [dataset.data[i] for i in ids]\n",
    "\n",
    "    # dataset.print_prepared_data()\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n",
    "                            num_workers=num_workers, collate_fn=dataset._collate_fn)\n",
    "    topk_answers = []\n",
    "    topk_scores = []\n",
    "    total_loss = 0\n",
    "    loader = tqdm(data_loader, total=len(data_loader), unit=\"batches\")\n",
    "    \n",
    "    \n",
    "    for i_batch, a in enumerate(loader):\n",
    "        # if size of split is multiple of batch size, we need this\n",
    "        # todo: is there a more elegant way?\n",
    "        if i_batch * batch_size == len(dataset.data):\n",
    "            break\n",
    "        answers_khot = a[-1] # last one assumed to be target\n",
    "        scores = qa_model.forward(a)\n",
    "        sm = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        scores = sm(scores)\n",
    "        # scores = torch.nn.functional.normalize(scores, p=2, dim=1)\n",
    "\n",
    "        for s in scores:\n",
    "            pred_s, pred = dataset.getAnswersFromScoresWithScores(s, k=max_k)\n",
    "            topk_answers.append(pred)\n",
    "            topk_scores.append(pred_s)\n",
    "        loss = qa_model.loss(scores, answers_khot.cuda())\n",
    "        total_loss += loss.item()\n",
    "    eval_log.append('Loss %f' % total_loss)\n",
    "    eval_log.append('Eval batch size %d' % batch_size)\n",
    "\n",
    "    for i in range(len(dataset.data)):\n",
    "        question = dataset.data[i]\n",
    "        predicted_answers = topk_answers[i]\n",
    "        predicted_scores = topk_scores[i]\n",
    "        actual_answers = question['answers']\n",
    "\n",
    "        if question['answer_type'] == 'entity':\n",
    "            actual_answers = [dataset.getEntityToText(x) for x in actual_answers]\n",
    "            pa = []\n",
    "            aa = []\n",
    "            for a in predicted_answers:\n",
    "                if 'Q' in str(a): # TODO: hack to check whether entity or time predicted\n",
    "                    pa.append(dataset.getEntityToText(a))\n",
    "                else:\n",
    "                    pa.append(a)\n",
    "            predicted_answers = pa\n",
    "\n",
    "            for a in actual_answers:\n",
    "                if 'Q' in str(a): # TODO: hack to check whether entity or time predicted\n",
    "                    aa.append(dataset.getEntityToText(a))\n",
    "                else:\n",
    "                    aa.append(a)\n",
    "            actual_answers = aa\n",
    "\n",
    "\n",
    "        # print(question['paraphrases'][0])\n",
    "        # print('Actual answers', actual_answers)\n",
    "        # print('Predicted answers', predicted_answers)\n",
    "        # print()\n",
    "        print(question['paraphrases'][0])\n",
    "        print(question['question'])\n",
    "        answers_with_scores_text = []\n",
    "        for pa, ps in zip(predicted_answers, predicted_scores):\n",
    "            formatted = '{answer} ({score})'.format(answer = pa, score=ps)\n",
    "            answers_with_scores_text.append(formatted)\n",
    "        print('Predicted:', ', '.join(answers_with_scores_text))\n",
    "        print('Actual:', ', '.join([str(x) for x in actual_answers]))\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntities(question_text):\n",
    "    words = question_text.split(' ')\n",
    "    entities = []\n",
    "    for word in words:\n",
    "        if word[0] == 'Q': # TODO: hack\n",
    "            entities.append(word)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = 'What is the name of the first team that Q1487425 was part of'\n",
    "entities = getEntities(question_text)\n",
    "times = []\n",
    "dataPoint = convertToDataPoint(question_text, entities, times)\n",
    "data = [dataPoint]\n",
    "valid_dataset.data = data\n",
    "valid_dataset.prepared_data = valid_dataset.prepare_data(data)\n",
    "\n",
    "predict(qa_model, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-vacation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 242308096/267967963 [01:16<00:04, 5946768.28B/s]"
     ]
    }
   ],
   "source": [
    "id2ent['Q1543']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "equal-vanilla",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the name of the last team that Q1487425 was part of',\n",
       "  'answers': {'Q1543'},\n",
       "  'answer_type': 'entity',\n",
       "  'template': 'What is the name of the last team that {head} was part of',\n",
       "  'entities': {'Q1487425'},\n",
       "  'times': set(),\n",
       "  'relations': {'P54'},\n",
       "  'type': 'first_last',\n",
       "  'annotation': {'head': 'Q1487425', 'adj': 'last'},\n",
       "  'uniq_id': 47,\n",
       "  'paraphrases': ['What is the name of the last team that Gianni Bui was part of']},\n",
       " {'question': 'Q1237590 received Q351723 in what year',\n",
       "  'answers': {2004},\n",
       "  'answer_type': 'time',\n",
       "  'template': '{head} received {tail} in what year',\n",
       "  'entities': {'Q1237590', 'Q351723'},\n",
       "  'times': set(),\n",
       "  'relations': {'P166'},\n",
       "  'type': 'simple_time',\n",
       "  'annotation': {'head': 'Q1237590', 'tail': 'Q351723'},\n",
       "  'uniq_id': 23016,\n",
       "  'paraphrases': ['Dominic Joyce received Adams Prize in what year']},\n",
       " {'question': 'Who played with Q1616158 on the Q8428',\n",
       "  'answers': {'Q1612196',\n",
       "   'Q1616158',\n",
       "   'Q16268364',\n",
       "   'Q257588',\n",
       "   'Q2824948',\n",
       "   'Q354475',\n",
       "   'Q3736398',\n",
       "   'Q3960871',\n",
       "   'Q739297',\n",
       "   'Q870487'},\n",
       "  'answer_type': 'entity',\n",
       "  'template': 'Who played with {head} on the {tail}',\n",
       "  'entities': {'Q1616158', 'Q8428'},\n",
       "  'times': set(),\n",
       "  'relations': {'P54'},\n",
       "  'type': 'time_join',\n",
       "  'annotation': {'head': 'Q1616158', 'tail': 'Q8428'},\n",
       "  'uniq_id': 42,\n",
       "  'paraphrases': ['Who played with Vittorio Coccia on the Calcio Padova']}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset.data[7:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "flexible-olive",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2ent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-ccbefa226c78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2ent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'id2ent' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "disturbed-library",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "important-avatar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "atomic-crisis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_text': ['Name the award that Bobby Clarke first received'],\n",
       " 'head': [120143],\n",
       " 'tail': [120143],\n",
       " 'time': [125726],\n",
       " 'answers_arr': [[]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset.prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "noted-sleeve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question_text', 'head', 'tail', 'time', 'answers_arr'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset.prepared_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "backed-compromise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Name the award that Q888504 first received',\n",
       " 'answers': {'Q795029'},\n",
       " 'answer_type': 'entity',\n",
       " 'template': 'Name the award that {head} first received',\n",
       " 'entities': {'Q888504'},\n",
       " 'times': set(),\n",
       " 'relations': {'P166'},\n",
       " 'type': 'first_last',\n",
       " 'annotation': {'head': 'Q888504', 'adj': 'first'},\n",
       " 'uniq_id': 23280,\n",
       " 'paraphrases': ['Name the award that Bobby Clarke first received']}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "civil-imagination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120143"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset.prepared_data['head'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "nutritional-aluminum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120143"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset.prepared_data['tail'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "exotic-torture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125726"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset.prepared_data['time'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "indie-upper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[116438]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset.prepared_data['answers_arr'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-consequence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
